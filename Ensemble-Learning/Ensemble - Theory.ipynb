{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition:\n",
    "\n",
    " Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model. There are many ensemble methods but we will look at the common ones.\n",
    " \n",
    "## Source:\n",
    "https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/ - has hyperparameter tuning for ensemble methods as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Voting Classifiers\n",
    "\n",
    "Suppose you have trained a few classifiers, each one achieving about 80% accuracy.\n",
    "You may have a Logistic Regression classifier, an SVM classifier, a Random Forest\n",
    "classifier, a K-Nearest Neighbors classifier.\n",
    "\n",
    "### Hard Voting: \n",
    "A very simple way to create an even better classifier is to aggregate the predictions of\n",
    "each classifier and predict the class that gets the most votes. This majority-vote classifier\n",
    "is called a hard voting classifier.\n",
    "\n",
    "This voting classifier often achieves a higher accuracy than the\n",
    "best classifier in the ensemble. In fact, even if each classifier is a <b>weak learner</b> (meaning\n",
    "it does only slightly better than random guessing), the ensemble can still be a\n",
    "<b>strong learner</b> (achieving high accuracy), provided there are a sufficient number of\n",
    "weak learners and they are sufficiently diverse.\n",
    "\n",
    "<img src=\"https://imgur.com/eAruUj3.png\" width=500>\n",
    "\n",
    "### Limitation of Voting classifiers:\n",
    "However, this is\n",
    "only true if all classifiers are perfectly independent, making uncorrelated errors,\n",
    "which is clearly not the case since they are trained on the same data. They are likely to\n",
    "make the same types of errors, so there will be many majority votes for the wrong\n",
    "class, reducing the ensemble’s accuracy.\n",
    "\n",
    "Ensemble methods work best when the <b>predictors are as independent</b>\n",
    "from one another as possible. One way to get diverse classifiers\n",
    "is to train them using very different algorithms. This increases the\n",
    "chance that they will make very different types of errors, improving\n",
    "the ensemble’s accuracy.\n",
    "\n",
    "### Soft Voting: \n",
    "If all classifiers are able to estimate class probabilities (i.e., they have a pre\n",
    "dict_proba() method), then you can tell Scikit-Learn to predict the class with the\n",
    "highest class probability, averaged over all the individual classifiers. This is called soft\n",
    "voting. It often achieves higher performance than hard voting because it gives more\n",
    "weight to highly confident votes.\n",
    "\n",
    "<img src=\"https://iq.opengenus.org/content/images/2020/01/ud382N9.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Bagging and Boosting Algorithms:\n",
    "\n",
    "Source: https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/\n",
    "\n",
    "1. Bagging algorithms:\n",
    "    - Bagging meta-estimator\n",
    "    - Random forest\n",
    "\n",
    "\n",
    "2. Boosting algorithms:\n",
    "    - AdaBoost\n",
    "    - GBM\n",
    "    - XGBM\n",
    "    - Light GBM\n",
    "    - CatBoost\n",
    "\n",
    "\n",
    "## 2. Bagging and Pasting\n",
    "\n",
    "One way to get a diverse set of classifiers is to use very different training algorithms,\n",
    "as just discussed. Another approach is to use the same training algorithm for every\n",
    "predictor, but to train them on different random subsets of the training set. When\n",
    "sampling is performed with replacement, this method is called <b>bagging</b> (short for\n",
    "bootstrap aggregating2). When sampling is performed without replacement, it is called\n",
    "<b>pasting</b>.\n",
    "\n",
    "\n",
    "## 2.1 Bagging (Boostrap aggregating)\n",
    "\n",
    "<b> What is a boostrap sample? </b>\n",
    "\n",
    "A bootstrap sample is a sample of a dataset with replacement. Replacement means that a sample drawn from the dataset is replaced, allowing it to be selected again and perhaps multiple times in the new sample. This means that the sample may have duplicate examples from the original dataset.\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRrFzT1XgY4ejMjo1VPelqCwiVNA9jPBxgPn84GmbXB-IED_2YbT3V8vRF5u9kXmJtSmMvuB9c3aCY1z496P8inPEiWRsIDgaRdYA&usqp=CAU&ec=45699845\" width=600>\n",
    "\n",
    "You can see in the above image that in the original dataset we only had 2 purple circles but in Boostrap sample 1 we have 3 instances of purple circles. This is possible due to replacement. After the first purple sample is picked, it is put back in the training set so there is a chance that it can get picked again (since it is getting selected randomly). Hence, a sample may have duplicate examples from original dataset\n",
    "\n",
    "<b> Why is it called boostrap aggregating?</b>\n",
    "\n",
    "The first step is to create bootstramp samples. Then you pass these samples to individual samples to the predictor(classifer or regressor). Finally, you aggregate the predictions of different classifiers to get the final result.\n",
    "\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/Bagging.png\" width=500>\n",
    "\n",
    "<b> The Aggregating Step:</b>\n",
    "\n",
    "Once all predictors are trained, the ensemble can make a prediction for a new\n",
    "instance by simply aggregating the predictions of all predictors. The aggregation\n",
    "function is typically the statistical mode (i.e., the most frequent prediction, just like a\n",
    "hard voting classifier) for classification, or the average for regression. Each individual\n",
    "predictor has a higher bias than if it were trained on the original training set, but\n",
    "aggregation reduces both bias and variance. \n",
    "\n",
    "The BaggingClassifier automatically performs soft voting\n",
    "instead of hard voting if the base classifier can estimate class probabilities\n",
    "(i.e., if it has a predict_proba() method).\n",
    "\n",
    "### Bagging or Pasting?\n",
    "\n",
    "Bootstrapping introduces a bit more diversity in the subsets that each predictor is\n",
    "trained on, so bagging ends up with a slightly higher bias than pasting, but this also\n",
    "means that predictors end up being less correlated so the ensemble’s variance is\n",
    "reduced. Overall, bagging often results in better models, which explains why it is generally\n",
    "preferred. However, if you have spare time and CPU power you can use crossvalidation\n",
    "to evaluate both bagging and pasting and select the one that works best.\n",
    "\n",
    "### Out of Bag (OOB) Evaluation:\n",
    "\n",
    "With bagging, some instances may be sampled several times for any given predictor,\n",
    "while others may not be sampled at all. By default a BaggingClassifier samples m\n",
    "training instances with replacement (bootstrap=True), where m is the size of the\n",
    "training set. This means that only about 63% of the training instances are sampled on\n",
    "average for each predictor. The remaining 37% of the training instances that are not\n",
    "sampled are called out-of-bag (oob) instances. Note that they are not the same 37%\n",
    "for all predictors.\n",
    "\n",
    "Since a predictor never sees the oob instances during training, it can be evaluated on\n",
    "these instances, without the need for a separate validation set or cross-validation. You\n",
    "can evaluate the ensemble itself by averaging out the oob evaluations of each predictor.\n",
    "In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier or even for RandomForest to\n",
    "request an automatic oob evaluation after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is another ensemble machine learning algorithm that follows the <b>bagging</b> technique. It is an extension of the bagging estimator algorithm. The base estimators in random forest are decision trees. Unlike bagging meta estimator, random forest randomly selects a set of features which are used to decide the best split at each node of the decision tree.\n",
    "\n",
    "<b> Steps:</b>\n",
    "\n",
    "1. Random subsets are created from the original dataset (bootstrapping).\n",
    "2. At each node in the decision tree, only a random set of features are considered to decide the best split.\n",
    "3. A decision tree model is fitted on each of the subsets.\n",
    "4. The final prediction is calculated by averaging the predictions from all decision trees.\n",
    "\n",
    "The Random Forest algorithm introduces extra randomness when growing trees;\n",
    "instead of searching for the very best feature when splitting a node, it\n",
    "searches for the best feature among a random subset of features. This results in a\n",
    "greater tree diversity, which (once again) trades a higher bias for a lower variance,\n",
    "generally yielding an overall better model.\n",
    "\n",
    "### Cost Function:\n",
    "\n",
    "Since it is an ensemble of Decision trees, by default it is Gini impurity. You can choose entropy as well.\n",
    "\n",
    "### Assumptions of Random Forest:\n",
    "\n",
    "No formal distributional assumptions, random forests are non-parametric and can thus handle skewed and multi-modal data as well as categorical data that are ordinal or non-ordinal.\n",
    "\n",
    "### Data preprocessing:\n",
    "\n",
    "Ideally, it doesn't need data preprocessing.\n",
    "\n",
    "<b> Don't one-hot encode for high cardinality datasets:</b>\n",
    "\n",
    "Decision tree models can handle categorical variables without one-hot encoding them. One-hot encoding categorical variables with high cardinality can cause inefficiency in tree-based ensembles. Continuous variables will be given more importance than the dummy variables by the algorithm which will obscure the order of feature importance resulting in poorer performance.\n",
    "\n",
    "<b> Feature selection will help for high dimensional data:</b>\n",
    "\n",
    "If you have high dimensions:\n",
    "The way to start would be to train a model with all the features and rank them according to feature importance by mean decrease impurity.\n",
    "\n",
    "Then, you can start removing features from the bottom of the list and evaluate the impact on performance.\n",
    "\n",
    "Since the random forest algorithm randomly selects features for its trees, having many irrelevant features will simply serve as noise and reduce performance. Normally, the model will perform better as more of them are removed.\n",
    "\n",
    "Continue the process until performance no longer improves, and you have your feature set.\n",
    "\n",
    "<b> Transformation helps:</b>\n",
    "\n",
    "Log-transformations can improve accuracy, especially in case of very skewed data (with very long tails). See for example \"Forecasting Bike Sharing Demand\" by Jayant Malani et al. (pdf), and [this kaggle submission](#https://www.kaggle.com/ademyttenaere/0-2748-with-rf-and-log-transformation/output).\n",
    "\n",
    "### Hyperparameter optimization:\n",
    "\n",
    "1. n_estimators:\n",
    "    - It defines the number of decision trees to be created in a random forest.\n",
    "    - Generally, a higher number makes the predictions stronger and more stable, but a very large number can result in higher training time.\n",
    "\n",
    "2. criterion:\n",
    "    - It defines the function that is to be used for splitting.\n",
    "    - The function measures the quality of a split for each feature and chooses the best split.\n",
    "\n",
    "3. max_features :\n",
    "    - It defines the maximum number of features allowed for the split in each decision tree.\n",
    "    - Increasing max features usually improve performance but a very high number can decrease the diversity of each tree.\n",
    "\n",
    "4. max_depth:\n",
    "    - Random forest has multiple decision trees. This parameter defines the maximum depth of the trees.\n",
    "\n",
    "5. min_samples_split:\n",
    "    - Used to define the minimum number of samples required in a leaf node before a split is attempted.\n",
    "    - If the number of samples is less than the required number, the node is not split.\n",
    "\n",
    "6. min_samples_leaf:\n",
    "    - This defines the minimum number of samples required to be at a leaf node.\n",
    "    - Smaller leaf size makes the model more prone to capturing noise in train data.\n",
    "\n",
    "7. max_leaf_nodes:\n",
    "    - This parameter specifies the maximum number of leaf nodes for each tree.\n",
    "    - The tree stops splitting when the number of leaf nodes becomes equal to the max leaf node.\n",
    "\n",
    "8. n_jobs:\n",
    "    - This indicates the number of jobs to run in parallel.\n",
    "    - Set value to -1 if you want it to run on all cores in the system.\n",
    "\n",
    "9. random_state:\n",
    "    - This parameter is used to define the random selection.\n",
    "    - It is used for comparison between various models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Overfitting:\n",
    "\n",
    "Source: https://stats.stackexchange.com/questions/460921/given-a-dataset-how-to-test-your-model-against-the-test-set-if-you-used-stratif\n",
    "\n",
    "1. n_estimators: In general the more trees the less likely the algorithm is to overfit. So try increasing this. The lower this number, the closer the model is to a decision tree, with a restricted feature set.\n",
    "2. max_features: try reducing this number (try 30-50% of the number of features). This determines how many features each tree is randomly assigned. The smaller, the less likely to overfit, but too small will start to introduce under fitting.\n",
    "3. max_depth: Experiment with this. This will reduce the complexity of the learned models, lowering over fitting risk. Try starting small, say 5-10, and increasing you get the best result.\n",
    "4. min_samples_leaf: Try setting this to values greater than one. This has a similar effect to the max_depth parameter, it means the branch will stop splitting once the leaves have that number of samples each.\n",
    "\n",
    "### Pros vs Cons:\n",
    "\n",
    "Source: https://www.quora.com/What-are-the-advantages-and-disadvantages-for-a-random-forest-algorithm\n",
    "\n",
    "#### Pros:\n",
    "\n",
    "1. Random forest can solve both type of problems that is classification and regression and does a decent estimation at both fronts.\n",
    "2. One of benefits of Random Forest which exists me most is, the power of handle large data sets with higher dimensionality. It can handle thousands of input variables and identity most significant variables so it is considered as one of the dimensionality reduction method. Further, the model outputs importance of variable, which can be a very handy feature.\n",
    "3. Good performance on many problems including non linear.\n",
    "4. Random Forest works well with both categorical and continuous variables.\n",
    "5. Random Forest is usually robust to outliers and can handle them automatically.\n",
    "\n",
    "#### Cons:\n",
    "\n",
    "1. No interpretability\n",
    "2. Overfitting can easily occur\n",
    "3.  Complexity: Random Forest creates a lot of trees (unlike only one tree in case of decision tree) and combines their outputs. By default, it creates 100 trees in Python sklearn library. To do so, this algorithm requires much more computational power and resources. On the other hand decision tree is simple and does not require so much computational resources.\n",
    "4. Longer Training Period: Random Forest require much more time to train as compared to decision trees as it generates a lot of trees (instead of one tree in case of decision tree) and makes decision on the majority of votes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boosting\n",
    "\n",
    "Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model.\n",
    "\n",
    "<img src=\"https://quantdare.com/wp-content/uploads/2016/04/bb3-800x307.png\">\n",
    "\n",
    "<b> Steps:</b>\n",
    "1. A subset is created from the original dataset.\n",
    "2. Initially, all data points are given equal weights.\n",
    "3. A base model is created on this subset.\n",
    "4. This model is used to make predictions on the whole dataset.\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2015/11/dd1-e1526989432375.png\">\n",
    "5. Errors are calculated using the actual values and predicted values.\n",
    "6. The observations which are incorrectly predicted, are given higher weights.\n",
    "(Here, the three misclassified blue-plus points will be given higher weights)\n",
    "7. Another model is created and predictions are made on the dataset.\n",
    "(This model tries to correct the errors from the previous model)\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2015/11/dd2-e1526989487878.png\">\n",
    "8. Similarly, multiple models are created, each correcting the errors of the previous model.\n",
    "9. The final model (strong learner) is the weighted mean of all the models (weak learners).\n",
    "<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2015/11/boosting10.png\">\n",
    "\n",
    "## 3.1 AdaBoost\n",
    "\n",
    "Adaptive boosting or AdaBoost is one of the simplest boosting algorithms. Usually, decision trees are used for modelling. Multiple sequential models are created, each correcting the errors from the last model. AdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly.\n",
    "\n",
    "### References:\n",
    "\n",
    "- https://youtu.be/LsK-xG1cLYA - Statsquest explains it well\n",
    "- https://youtu.be/NLRO1-jp5F8 - Explained with an example and calculations\n",
    "\n",
    "<b> Steps:</b>\n",
    "1. Initially, all observations in the dataset are given equal weights.\n",
    "2. A model is built on a subset of data.\n",
    "3. Using this model, predictions are made on the whole dataset.\n",
    "4. Errors are calculated by comparing the predictions and actual values.\n",
    "5. While creating the next model, higher weights are given to the data points which were predicted incorrectly.\n",
    "6. Weights can be determined using the error value. For instance, higher the error more is the weight assigned to the observation.\n",
    "7. This process is repeated until the error function does not change, or the maximum limit of the number of estimators is reached.\n",
    "\n",
    "### Hyperparameter optimization:\n",
    "\n",
    "1. base_estimators:\n",
    "    - It helps to specify the type of base estimator, that is, the machine learning algorithm to be used as base learner.\n",
    "    - SVMs are generally not good base predictors for AdaBoost, because they\n",
    "are slow and tend to be unstable with AdaBoost.\n",
    "\n",
    "2. n_estimators:\n",
    "    - It defines the number of base estimators.\n",
    "    - The default value is 10, but you should keep a higher value to get better performance.\n",
    "\n",
    "3. learning_rate:\n",
    "    - This parameter controls the contribution of the estimators in the final combination.\n",
    "    - There is a trade-off between learning_rate and n_estimators.\n",
    "\n",
    "4. max_depth:\n",
    "    - Defines the maximum depth of the individual estimator.\n",
    "    - Tune this parameter for best performance.\n",
    "\n",
    "5. n_jobs\n",
    "    - Specifies the number of processors it is allowed to use.\n",
    "    - Set value to -1 for maximum processors allowed.\n",
    "\n",
    "6. random_state :\n",
    "    - An integer value to specify the random data split.\n",
    "    - A definite value of random_state will always produce same results if given with same parameters and training data.\n",
    "    \n",
    "### Solutions to overfitting:\n",
    "\n",
    "If your AdaBoost ensemble is overfitting the training set, you can\n",
    "try reducing the number of estimators or more strongly regularizing\n",
    "the base estimator.\n",
    "\n",
    "1. n_estimators: In general the more trees the less likely the algorithm is to overfit. So try increasing this. The lower this number, the closer the model is to a decision tree, with a restricted feature set.\n",
    "\n",
    "2. max_depth: Experiment with this. This will reduce the complexity of the learned models, lowering over fitting risk. Try starting small, say 5-10, and increasing you get the best result.\n",
    "\n",
    "### Pros vs Cons:\n",
    "\n",
    "#### Pros:\n",
    "- Very simple to implement\n",
    "- Does feature selection resulting in relatively\n",
    "simple classifier\n",
    "- Fairly good generalization\n",
    "\n",
    "#### Cons:\n",
    "- Suboptimal solution\n",
    "- Sensitive to noisy data and outliers which can cause overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Gradient Boosting\n",
    "\n",
    "Just like AdaBoost,\n",
    "Gradient Boosting works by sequentially adding predictors to an ensemble, each one\n",
    "correcting its predecessor. However, instead of tweaking the instance weights at every\n",
    "iteration like AdaBoost does, this method tries to fit the new predictor to the residual\n",
    "errors made by the previous predictor.\n",
    "\n",
    "### References\n",
    "- https://youtu.be/Oo9q6YtGzvc - Math behind it\n",
    "\n",
    "### Hyperparameter Optimization:\n",
    "\n",
    "1. learning_rate:\n",
    "\n",
    "    - The learning_rate hyperparameter scales the contribution of each tree. If you set it\n",
    "to a low value, such as 0.1, you will need more trees in the ensemble to fit the training\n",
    "set, but the predictions will usually generalize better. This is a regularization technique\n",
    "called <b>shrinkage.</b>\n",
    "\n",
    "2. n_estimators:\n",
    "\n",
    "    - In order to find the optimal number of trees, you can use early stopping. A simple way to implement this is to use the <b>staged_predict()</b> method: it\n",
    "returns an iterator over the predictions made by the ensemble at each stage of training\n",
    "(with one tree, two trees, etc.). The following code trains a GBRT ensemble with\n",
    "120 trees, then measures the validation error at each stage of training to find the optimal\n",
    "number of trees, and finally trains another GBRT ensemble using the optimal\n",
    "number of trees\n",
    "\n",
    "   - Code: \n",
    "   \n",
    "    import numpy as np\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "    \n",
    "    gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "    \n",
    "    gbrt.fit(X_train, y_train)\n",
    "    \n",
    "    errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
    "    \n",
    "    bst_n_estimators = np.argmin(errors)\n",
    "    \n",
    "    gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "    \n",
    "    gbrt_best.fit(X_train, y_train)\n",
    "    \n",
    "3. min_samples_split:\n",
    "\n",
    "    - Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    \n",
    "4. max_depth:\n",
    "\n",
    "    - The maximum depth of a tree.\n",
    "    - Used to control over-fitting as higher depth will allow the model to learn relations very specific to a particular sample.\n",
    "    \n",
    "5. max_features:\n",
    "\n",
    "    - The number of features to consider while searching for the best split. These will be randomly selected.\n",
    "    - As a thumb-rule, the square root of the total number of features works great but we should check up to 30-40% of the total number of features.\n",
    "    - Higher values can lead to over-fitting but it generally depends on a case to case scenario.\n",
    "    \n",
    "6. subsample:\n",
    "\n",
    "    - The GradientBoostingRegressor class also supports a subsample hyperparameter,\n",
    "which specifies the fraction of training instances to be used for training each tree. For\n",
    "example, if subsample=0.25, then each tree is trained on 25% of the training instances,\n",
    "selected randomly. As you can probably guess by now, this trades a higher bias\n",
    "for a lower variance. It also speeds up training considerably. This technique is called\n",
    "<b>Stochastic Gradient Boosting.</b>\n",
    "\n",
    "### Solution to overfitting:\n",
    "\n",
    "Source: https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "General approach-\n",
    "\n",
    "- Choose a relatively high learning rate. Generally the default value of 0.1 works but somewhere between 0.05 to 0.2 should work for different problems\n",
    "- Determine the optimum number of trees for this learning rate. This should range around 40-70. Remember to choose a value on which your system can work fairly fast. This is because it will be used for testing various scenarios and determining the tree parameters.\n",
    "- Tune tree-specific parameters for decided learning rate and number of trees. Note that we can choose different parameters to define a tree and I’ll take up an example here.\n",
    "- Lower the learning rate and increase the estimators proportionally to get more robust models.\n",
    "\n",
    "### Pros vs Cons:\n",
    "\n",
    "#### Pros:\n",
    "\n",
    "- Often provides predictive accuracy that cannot be beat.\n",
    "- Lots of flexibility - can optimize on different loss functions and provides several hyperparameter tuning options that make the function fit very flexible.\n",
    "- No data pre-processing required - often works great with categorical and numerical values as is.\n",
    "- Handles missing data - imputation not required.\n",
    "\n",
    "#### Cons:\n",
    "\n",
    "- GBMs will continue improving to minimize all errors. This can overemphasize outliers and cause <b>overfitting</b>. Must use cross-validation to neutralize.\n",
    "- Computationally expensive - GBMs often require many trees (>1000) which can be time and memory exhaustive.\n",
    "- The high flexibility results in many parameters that interact and influence heavily the behavior of the approach (number of iterations, tree depth, regularization parameters, etc.). This requires a large grid search during tuning.\n",
    "- Less interpretable although this is easily addressed with various tools (variable importance, partial dependence plots, LIME, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 XGBoost\n",
    "\n",
    "XGBoost (extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. XGBoost has proved to be a highly effective ML algorithm, extensively used in machine learning competitions and hackathons. XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known as ‘regularized boosting‘ technique.\n",
    "\n",
    "Let us see how XGBoost is comparatively better than other techniques:\n",
    "\n",
    "1. Regularization:\n",
    "    - Standard GBM implementation has no regularisation like XGBoost.\n",
    "    - Thus XGBoost also helps to reduce overfitting.\n",
    "\n",
    "2. Parallel Processing:\n",
    "    - XGBoost implements parallel processing and is faster than GBM .\n",
    "    - XGBoost also supports implementation on Hadoop.\n",
    "\n",
    "3. High Flexibility:\n",
    "    - XGBoost allows users to define custom optimization objectives and evaluation criteria adding a whole new dimension to the model.\n",
    "\n",
    "4. Handling Missing Values:\n",
    "    - XGBoost has an in-built routine to handle missing values.\n",
    "\n",
    "4. Tree Pruning:\n",
    "    - XGBoost makes splits up to the max_depth specified and then starts pruning the tree backwards and removes splits beyond which there is no positive gain.\n",
    "\n",
    "5. Built-in Cross-Validation:\n",
    "    - XGBoost allows a user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.\n",
    "    \n",
    "### Hyperparameter Optimization:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "\n",
    "1. min_child_weight:\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "\n",
    "2. max_depth\n",
    "    - It is used to define the maximum depth.\n",
    "    - Higher depth will allow the model to learn relations very specific to a particular sample.\n",
    "\n",
    "3. max_leaf_nodes\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "\n",
    "4. gamma\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "\n",
    "4. subsample\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly sampled for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevent overfitting but values that are too small might lead to under-fitting.\n",
    "    \n",
    "### Solution to Overfitting:\n",
    "\n",
    "Source: https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html\n",
    "\n",
    "There are in general two ways that you can control overfitting in XGBoost:\n",
    "\n",
    "1. The first way is to directly control model complexity.\n",
    "\n",
    "    - This includes max_depth, min_child_weight and gamma.\n",
    "\n",
    "2. The second way is to add randomness to make training robust to noise.\n",
    "\n",
    "    - This includes subsample and colsample_bytree.\n",
    "\n",
    "    - You can also reduce stepsize eta. Remember to increase num_round when you do so.\n",
    "    \n",
    "### Pros vs Cons:\n",
    "\n",
    "Source:https://towardsdatascience.com/pros-and-cons-of-various-classification-ml-algorithms-3b5bfb3c87d6\n",
    "\n",
    "#### Pros:\n",
    "1. Less feature engineering required (No need for scaling, normalizing data, can also handle missing values well)\n",
    "2. Feature importance can be found out(it output importance of each feature, can be used for feature selection)\n",
    "3. Fast to interpret\n",
    "4. Outliers have minimal impact.\n",
    "5. Handles large sized datasets well.\n",
    "6. Good Execution speed\n",
    "7. Good model performance (wins most of the Kaggle competitions)\n",
    "8. Less prone to overfitting\n",
    "\n",
    "#### Cons:\n",
    "1. Difficult interpretation , visualization tough\n",
    "2. Overfitting possible if parameters not tuned properly.\n",
    "3. Harder to tune as there are too many hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
